Could The Transformer be another nail in the coffin for RNNs? Doing away with the clunky for loops, it finds a way to allow whole sentences to simultaneously enter the network in batches. The miracle; NLP now reclaims the advantage of python’s highly efficient linear algebra libraries. This time-saving can then spent deploying more layers into the model. So far it seems the result is faster convergence and better results. What’s not to love? My personal experience of it has been highly promising. It trained on 2 million French-English sentence pairs to create a sophisticated translator in only three days.
